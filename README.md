# ETL-databricks-pyspark
These databricks notebooks contains the guidance for ETL for big data. Raw data is first collected from the data lake using databricks which are mounted datalake with the help of spark read API, then different transformations are applied using Spark dataframe API's and also pyspark.sql functions, then after transformation the data (parquet file) is loaded into the datalake again using Spark write API's. 
